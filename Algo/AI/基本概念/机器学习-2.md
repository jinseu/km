

#### 强化学习




### 卷积

**简介**

卷积首先是一个数学概念，在泛函分析中，卷积（convolution）是透过两个函数 f 和 g 生成第三个函数的一种数学算子，表征函数 f 与经过翻转和平移的 g 的乘积函数所围成的曲边梯形的面积。如果将参加卷积的一个函数看作区间的指示函数，卷积还可以被看作是“移动平均”的推广。

卷积是数学分析中一种重要的运算。设：f(x), g(x) 是两个可积函数，

[!卷积](./img/convolution.svg)

这个积分就定义了一个新函数h(x)，称为函数f(x)，g(x)的卷积。对于卷积的理解，可以采用如下方式

- 已知两函数f(t)和g(t)。
- 首先将两个函数都用x来表示，从而得到f(x)和g(x)。将函数g(x)向左移动t个单位，得到函数g(x+t)的图像。将g(x+t)翻转至纵轴另一侧，得到g(t-x)的图像。
- 由于t非常数（实际上是时间变量），当时间变量（以下简称“时移”）取不同值时，g(t-x)能沿着x轴“滑动”。
- 让x从-∞滑动到+∞。两函数交会时，计算交会范围中两函数乘积的积分值。换句话说，我们是在计算一个滑动的的加权总和(weighted-sum)。也就是使用g(t-x)当做加权函数，来对f(x)取加权值。


在深度学习中，通过卷积实现前后两层神经元的稀疏链接，由于卷积核一般较小，相对而言权重数目也比较少，可以有效减少训练参数的数目，加快模型的训练和收敛速度。

**例子**

下面的例子中通过卷积实现一个sedol滤波器，来得到一幅图片边缘。

```Python
#!/usr/bin/env python
# coding: utf-8
import torch
import numpy as np
from torch import nn
from PIL import Image
from torch.autograd import Variable
import torch.nn.functional as F
import cv2
im = cv2.imread('./cat.jpeg', flags=1)
im = np.transpose(im, (2, 0, 1))
im = im[np.newaxis, :]
im = torch.Tensor(im)
# im shape torch.Size([1, 3, 400, 600])
conv_op = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=False)
sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') / 3
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3))
sobel_kernel = np.repeat(sobel_kernel, 3, axis=1)
sobel_kernel = np.repeat(sobel_kernel, 3, axis=0)
conv_op.weight.data = torch.from_numpy(sobel_kernel)
edge_detect = conv_op(im)
edge_detect = edge_detect.squeeze().detach().numpy()
edge_detect = np.transpose(edge_detect, (1, 2, 0))
cv2.imwrite('edge-cat.jpeg', edge_detect)
```

需要说明的是

1. 这里在`Conv2d`函数中设置的inchannel，outchannel 都为3，所以对应卷积核为`[3, 3, 3, 3]`，即` N = outchannel, C = inchannel，H = kernel_size W = kernel_size`，这是因为每次参与卷积运算的数据大小为`C*H*W`，为了得到outchannel，需要将卷积核的batch_size设置为outchannel，即设置outchannel个卷积核，从而最终得到得到输出。
2. 要区分卷积核的N和输入输出的N


### 损失函数

损失函数或代价函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数，借此直观表示的一些"成本"与事件的关联。一个最佳化问题的目标是将损失函数最小化。

A loss function is a part of a cost function which is a type of an objective function

### 常见的损失函数

#### 0-1 损失函数


#### 交叉熵损失函数

#### 平方损失函数

平方损失函数十分常见，比如用在最小二乘法中。它在数学上通常比其他损失函数更容易进行处理，这是因为它具有方差的性质，以及对称性：高于目标值的误差产生的损失与低于目标值同样大小的误差产生的损失相等。假设目标值为t，那么平方损失函数为`C(t-x)^2`

#### 回归模型和分类模型常用损失函数有哪些？各有什么优缺点

#### 为什么分类问题不用平方损失函数?

均方差损失的时候讲到实际上均方差损失假设了误差服从高斯分布，在分类任务下，如果使用最小均方差Loss时，模型参数w会学习的非常慢。而使用交叉熵Loss则没有这个问题。为了更快的学习速度，分类问题一般采用交叉熵损失函数。

> https://zhuanlan.zhihu.com/p/25723112
> https://blog.csdn.net/u014313009/article/details/51043064
> https://blog.csdn.net/liuweiyuxiang/article/details/90707375