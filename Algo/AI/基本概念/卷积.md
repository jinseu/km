## 卷积

### 简介

卷积首先是一个数学概念，在泛函分析中，卷积（convolution）是透过两个函数 f 和 g 生成第三个函数的一种数学算子，表征函数 f 与经过翻转和平移的 g 的乘积函数所围成的曲边梯形的面积。如果将参加卷积的一个函数看作区间的指示函数，卷积还可以被看作是“移动平均”的推广。

卷积是数学分析中一种重要的运算。设：f(x), g(x) 是两个可积函数，

[!卷积](./img/convolution.svg)

这个积分就定义了一个新函数h(x)，称为函数f(x)，g(x)的卷积。对于卷积的理解，可以采用如下方式

- 已知两函数f(t)和g(t)。
- 首先将两个函数都用x来表示，从而得到f(x)和g(x)。将函数g(x)向左移动t个单位，得到函数g(x+t)的图像。将g(x+t)翻转至纵轴另一侧，得到g(t-x)的图像。
- 由于t非常数（实际上是时间变量），当时间变量（以下简称“时移”）取不同值时，g(t-x)能沿着x轴“滑动”。
- 让x从-∞滑动到+∞。两函数交会时，计算交会范围中两函数乘积的积分值。换句话说，我们是在计算一个滑动的的加权总和(weighted-sum)。也就是使用g(t-x)当做加权函数，来对f(x)取加权值。


### 应用

在深度学习中，通过卷积实现前后两层神经元的稀疏链接，由于卷积核一般较小，相对而言权重数目也比较少，可以有效减少训练参数的数目，加快模型的训练和收敛速度。

在机器学习场景下，卷积的使用和数学定义稍有不同，需要注意

1. 卷积核中的权重值是在训练过程中产生的，并随着训练的进行而变化，所以一般而言不会直接设置卷积核的值。直接设置卷积核的例子可以参见`滤波器`
2. 卷积层在训练时，对于卷积核的大小只能设置`[H,W]`，其`N`（batch_size)由`outchannel`的值决定，`C`(channel)由`inchannel`的值决定。即实际训练时的卷积核不是一个，而是一组。
3. 在实际计算时，卷积核在空间维度（空间大小为`[H,W]`）与输入张量空间维度对应位置相乘，然后将所有`channel`同一空间维度的值叠加，得到输出的一个`channel`，N个卷积核对应得到N个`channel`，将N个`channel`合并到一起，即得到了输出张量。

#### 滤波器

下面的例子中通过卷积实现一个sedol滤波器，来得到一幅图片边缘。

```Python
#!/usr/bin/env python
# coding: utf-8
import torch
import numpy as np
from torch import nn
from PIL import Image
from torch.autograd import Variable
import torch.nn.functional as F
import cv2
im = cv2.imread('./cat.jpeg', flags=1)
im = np.transpose(im, (2, 0, 1))
im = im[np.newaxis, :]
im = torch.Tensor(im)
# im shape torch.Size([1, 3, 400, 600])
conv_op = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=False)
sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') / 3
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3))
sobel_kernel = np.repeat(sobel_kernel, 3, axis=1)
sobel_kernel = np.repeat(sobel_kernel, 3, axis=0)
conv_op.weight.data = torch.from_numpy(sobel_kernel)
edge_detect = conv_op(im)
edge_detect = edge_detect.squeeze().detach().numpy()
edge_detect = np.transpose(edge_detect, (1, 2, 0))
cv2.imwrite('edge-cat.jpeg', edge_detect)
```

### 备注

#### `1*1`卷积核的作用

在`Conv2d`函数中设置的inchannel，outchannel 都为3，所以对应卷积核为`[3, 3, 3, 3]`，即` N = outchannel, C = inchannel，H = kernel_size W = kernel_size`，这是因为每次参与卷积运算的数据大小为`C*H*W`，为了得到outchannel，需要将卷积核的`batch_size`设置为outchannel，即设置outchannel个卷积核，从而最终得到得到输出。此处需要注意区别卷积核的N和输入输出的N.

于是在卷积核为`1*1`时，卷积操作相当于将不同通道，同一空间位置的数据加起来，从而改变输入数据的通道数，实现通道数目的变换。


#### 分组卷积的作用

分组卷积，是将输入张量分为`groups`个组，最后把每组卷积输出结果沿着通道进行拼装。在分组卷积时，会将输入张量分为`groups`个组，每组大小为`[N,inchannel/groups, H, W]`，每组的输出大小为`[N, outchannel/groups, H, W]`，然后将所有分组合并在一起得到输出张量`[N, outchannel, H, W]`。

使用分组卷积的主要目的是加快计算速度，在不分组的情况下：

- 参数量 `inchannel*H*W*outchannel`

分组的情况下

- 参数量 每组参数为`(inchannel/g)*H*W*(outchannel/g)`，合计为`(inchannel/g)*H*W*(outchannel/g)*g = (inchannel*H*W*outchannel*(1/g)`，参数量变为`1/g`

可以看到在分组的情况下，大大减少了参数量，提高了计算速度，但是需要注意的是，分组之后，输出的一个Channel空间，只受对应分组的Channel决定，不存在所有Channel的全连接，可能会导致某些特征无法被识别。

当`inchannel = outchannel = groups`时，称为深度分离卷积。

注意，使用分组卷积，要求inchannel参数和outchannel参数都能被groups参数整除。

> https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Group_Convolution.html